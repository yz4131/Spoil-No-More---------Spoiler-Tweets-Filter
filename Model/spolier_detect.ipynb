{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820321dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::176363299110:role/service-role/AmazonSageMaker-ExecutionRole-20220425T104815\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "bucket_name = 'elen6889'\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_key_prefix = 'spoiler-classifier'\n",
    "vocabulary_length = 9013\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3020a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7f8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset/archive.zip\n",
      "  inflating: dataset/IMDB_movie_details.json  \n",
      "  inflating: dataset/IMDB_reviews.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o dataset/archive.zip -d dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f23b13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from classifier_utilities import one_hot_encode\n",
    "from classifier_utilities import vectorize_sequences\n",
    "#df_reviews = pd.read_json('dataset/IMDB_reviews.json', lines=True)\n",
    "df_review = pd.read_csv('dataset/dfnew.csv')\n",
    "#print('User reviews shape: ', df_reviews.shape)\n",
    "#df[df.columns[0]] = df[df.columns[0]].map({'True': 0, 'False': 1})\n",
    "df = pd.DataFrame()\n",
    "df['is_spoiler'] = df_review['is_spoiler']\n",
    "df['text'] = df_review['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b19d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[0]] = df[df.columns[0]].map({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3f9417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cd2a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>1</td>\n",
       "      <td>Shawshank has everything you need from a movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>1</td>\n",
       "      <td>Freeman, who is simply a great actor, a man wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @chrissvellx: the ending of #MoonKnight epi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_spoiler                                               text\n",
       "1289           1  Shawshank has everything you need from a movie...\n",
       "2376           1  Freeman, who is simply a great actor, a man wh...\n",
       "307            0  RT @chrissvellx: the ending of #MoonKnight epi..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "586ccc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df[df.columns[0]].values\n",
    "messages = df[df.columns[1]].values\n",
    "\n",
    "# one hot encoding for each SMS message\n",
    "one_hot_data = one_hot_encode(messages, vocabulary_length)\n",
    "encoded_messages = vectorize_sequences(one_hot_data, vocabulary_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7412d9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think this work is very important! I think it will go a long way in the future, so hopefully the project will beâ€¦ https://t.co/tlrI78Xgog'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3c121a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f87ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(encoded_messages)\n",
    "df2.insert(0, 'spam', targets)\n",
    "\n",
    "# Split into training and validation sets (80%/20% split)\n",
    "split_index = int(np.ceil(df.shape[0] * 0.8))\n",
    "train_set = df2[:split_index]\n",
    "val_set = df2[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4d7fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('dataset/sms_train_set.gz', header=False, index=False, compression='gzip')\n",
    "val_set.to_csv('dataset/sms_val_set.gz', header=False, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31a93524",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('dataset/sms_train_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/train/sms_train_set.gz'.format(bucket_key_prefix))\n",
    "    \n",
    "with open('dataset/sms_val_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/val/sms_val_set.gz'.format(bucket_key_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d5ef182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "import pip\r\n",
      "\r\n",
      "try:\r\n",
      "    from pip import main as pipmain\r\n",
      "except:\r\n",
      "    from pip._internal import main as pipmain\r\n",
      "\r\n",
      "pipmain(['install', 'pandas'])\r\n",
      "import pandas\r\n",
      "\r\n",
      "#logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "          num_gpus, num_cpus, hosts, current_host, **kwargs):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters and apply some defaults in case they are not provided.\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "\r\n",
      "    train_data_path = channel_input_dirs['train']\r\n",
      "    val_data_path = channel_input_dirs['val']\r\n",
      "    train_data = get_train_data(train_data_path, batch_size)\r\n",
      "    val_data = get_val_data(val_data_path, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Normal(sigma=1.), ctx=ctx)\r\n",
      "    \r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    \r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        \r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            \r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "            L.backward()\r\n",
      "\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "\r\n",
      "            # update metric at last.\r\n",
      "            sigmoid_output = output.sigmoid() \r\n",
      "            prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "            metric.update([label], [prediction])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(64, activation=\"relu\"))\r\n",
      "        net.add(nn.Dense(1))\r\n",
      "    return net\r\n",
      "\r\n",
      "def get_train_data(data_path, batch_size):\r\n",
      "    print('Train data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_train_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=True)\r\n",
      "\r\n",
      "def get_val_data(data_path, batch_size):\r\n",
      "    print('Validation data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_val_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        \r\n",
      "        output = net(data)\r\n",
      "        sigmoid_output = output.sigmoid() \r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        metric.update([label], [prediction])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    net = gluon.nn.SymbolBlock(\r\n",
      "        outputs=mx.sym.load('%s/model.json' % model_dir),\r\n",
      "        inputs=mx.sym.var('data'))\r\n",
      "    \r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    try:\r\n",
      "        parsed = json.loads(data)\r\n",
      "        nda = mx.nd.array(parsed)\r\n",
      "        \r\n",
      "        output = net(nda)\r\n",
      "        sigmoid_output = output.sigmoid()\r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        output_obj = {}\r\n",
      "        output_obj['predicted_label'] = prediction.asnumpy().tolist()\r\n",
      "        output_obj['predicted_probability'] = sigmoid_output.asnumpy().tolist()\r\n",
      "\r\n",
      "        response_body = json.dumps(output_obj)\r\n",
      "        return response_body, output_content_type\r\n",
      "    except Exception as ex:\r\n",
      "        response_body = '{error: }' + str(ex)\r\n",
      "        return response_body, output_content_type\r\n",
      "    "
     ]
    }
   ],
   "source": [
    "!cat 'sms_spam_classifier_mxnet_script.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "202d2b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-05 02:23:13 Starting - Starting the training job...\n",
      "2022-05-05 02:23:38 Starting - Preparing the instances for trainingProfilerReport-1651717392: InProgress\n",
      "......\n",
      "2022-05-05 02:24:38 Downloading - Downloading input data...\n",
      "2022-05-05 02:25:02 Training - Training image download completed. Training in progress.\u001b[34m2022-05-05 02:25:04,136 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2022-05-05 02:25:04,137 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2022-05-05 02:25:04,141 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34m2022-05-05 02:25:04,980 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[34m2022-05-05 02:25:04,987 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'_scheduler_ip': '10.2.194.220', 'available_gpus': 0, 'enable_cloudwatch_metrics': False, 'channels': {'val': {'TrainingInputMode': 'File', 'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None'}, 'train': {'TrainingInputMode': 'File', 'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None'}}, 'sagemaker_region': 'us-east-1', 'user_script_archive': 's3://elen6889/spoiler-classifier/code/sms-spam-classifier-mxnet-2022-05-05-02-23-12-709/source/sourcedir.tar.gz', 'output_dir': '/opt/ml/output', 'code_dir': '/opt/ml/code', 'input_dir': '/opt/ml/input', 'current_host': 'algo-1', 'base_dir': '/opt/ml', 'output_data_dir': '/opt/ml/output/data/', '_scheduler_host': 'algo-1', '_ps_port': 8000, 'resource_config': {'hosts': ['algo-1'], 'current_instance_type': 'ml.c5.2xlarge', 'network_interface_name': 'eth0', 'current_host': 'algo-1', 'current_group_name': 'homogeneousCluster', 'instance_groups': [{'instance_type': 'ml.c5.2xlarge', 'hosts': ['algo-1'], 'instance_group_name': 'homogeneousCluster'}]}, 'model_dir': '/opt/ml/model', 'hosts': ['algo-1'], 'user_script_name': 'sms_spam_classifier_mxnet_script.py', 'job_name': 'sms-spam-classifier-mxnet-2022-05-05-02-23-12-709', 'channel_dirs': {'val': '/opt/ml/input/data/val', 'train': '/opt/ml/input/data/train'}, 'input_config_dir': '/opt/ml/input/config', 'user_requirements_file': None, 'hyperparameters': {'learning_rate': 0.3, 'sagemaker_job_name': 'sms-spam-classifier-mxnet-2022-05-05-02-23-12-709', 'epochs': 10, 'batch_size': 300, 'sagemaker_program': 'sms_spam_classifier_mxnet_script.py', 'sagemaker_submit_directory': 's3://elen6889/spoiler-classifier/code/sms-spam-classifier-mxnet-2022-05-05-02-23-12-709/source/sourcedir.tar.gz', 'sagemaker_region': 'us-east-1', 'sagemaker_container_log_level': 20}, 'container_log_level': 20, '_ps_verbose': 0, 'available_cpus': 8}\u001b[0m\n",
      "\u001b[34mDownloading s3://elen6889/spoiler-classifier/code/sms-spam-classifier-mxnet-2022-05-05-02-23-12-709/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2022-05-05 02:25:05,279 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[34mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas)\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/60/2e/dec1cc18c51b8df33c7c4d0a321b084cf38e1733b98f9d15018880fb4970/pytz-2022.1-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2022.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:190: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[34mTrain data path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mValidation data path: /opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34m[Epoch 0] Training: accuracy=0.703461\u001b[0m\n",
      "\u001b[34m[Epoch 0] Validation: accuracy=0.723750\u001b[0m\n",
      "\u001b[34m[Epoch 1] Training: accuracy=0.838790\u001b[0m\n",
      "\u001b[34m[Epoch 1] Validation: accuracy=0.901250\u001b[0m\n",
      "\u001b[34m[Epoch 2] Training: accuracy=0.920798\u001b[0m\n",
      "\u001b[34m[Epoch 2] Validation: accuracy=0.933750\u001b[0m\n",
      "\u001b[34m[Epoch 3] Training: accuracy=0.946056\u001b[0m\n",
      "\u001b[34m[Epoch 3] Validation: accuracy=0.906250\u001b[0m\n",
      "\u001b[34m[Epoch 4] Training: accuracy=0.952915\u001b[0m\n",
      "\u001b[34m[Epoch 4] Validation: accuracy=0.940000\u001b[0m\n",
      "\u001b[34m[Epoch 5] Training: accuracy=0.974119\u001b[0m\n",
      "\u001b[34m[Epoch 5] Validation: accuracy=0.937500\u001b[0m\n",
      "\u001b[34m[Epoch 6] Training: accuracy=0.980667\u001b[0m\n",
      "\u001b[34m[Epoch 6] Validation: accuracy=0.952500\u001b[0m\n",
      "\u001b[34m[Epoch 7] Training: accuracy=0.983162\u001b[0m\n",
      "\u001b[34m[Epoch 7] Validation: accuracy=0.941250\u001b[0m\n",
      "\u001b[34m[Epoch 8] Training: accuracy=0.983162\u001b[0m\n",
      "\u001b[34m[Epoch 8] Validation: accuracy=0.956250\u001b[0m\n",
      "\u001b[34m[Epoch 9] Training: accuracy=0.987839\u001b[0m\n",
      "\u001b[34m[Epoch 9] Validation: accuracy=0.952500\u001b[0m\n",
      "\n",
      "2022-05-05 02:25:38 Uploading - Uploading generated training model\n",
      "2022-05-05 02:25:58 Completed - Training job completed\n",
      "Training seconds: 72\n",
      "Billable seconds: 72\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "output_path = 's3://{0}/{1}/output'.format(bucket_name, bucket_key_prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, bucket_key_prefix)\n",
    "\n",
    "m = MXNet('sms_spam_classifier_mxnet_script.py',\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          instance_type='ml.c5.2xlarge',\n",
    "          output_path=output_path,\n",
    "          base_job_name='sms-spam-classifier-mxnet',\n",
    "          framework_version=\"1.2\",\n",
    "          py_version=\"py3\",\n",
    "          code_location = code_location,\n",
    "          hyperparameters={'batch_size': 300,\n",
    "                         'epochs': 10,\n",
    "                         'learning_rate': 0.3})\n",
    "\n",
    "inputs = {'train': 's3://{0}/{1}/train/'.format(bucket_name, bucket_key_prefix),\n",
    " 'val': 's3://{0}/{1}/val/'.format(bucket_name, bucket_key_prefix)}\n",
    "\n",
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b83fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "mxnet_pred = m.deploy(initial_instance_count=1,\n",
    "                      instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e5c432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "{'predicted_label': [[1.0]], 'predicted_probability': [[0.9983275532722473]]}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet.model import MXNetPredictor\n",
    "from classifier_utilities import one_hot_encode\n",
    "from classifier_utilities import vectorize_sequences\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "mxnet_pred = MXNetPredictor('sms-spam-classifier-mxnet-2022-05-05-02-26-26-135')\n",
    "\n",
    "test_messages = ['''RT @ZenitsuStreams: God pls dont spoil @\n",
    "#DoctorStrange #MultiverseOfMadness #Wanda https://t.co/UJYmar6kYb''']\n",
    "one_hot_test_messages = one_hot_encode(test_messages, vocabulary_length)\n",
    "encoded_test_messages = vectorize_sequences(one_hot_test_messages, vocabulary_length)\n",
    "\n",
    "print(encoded_test_messages)\n",
    "\n",
    "result = mxnet_pred.predict(encoded_test_messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1612b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350901a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
